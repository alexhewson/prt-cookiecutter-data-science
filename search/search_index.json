{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Prison Reform Trust Cookiecutter Data Science","text":"<p>A logical, reasonably standardized, but flexible project structure for doing and sharing data science work\u2014adapted for Prison Reform Trust.</p>"},{"location":"#getting-started","title":"Getting started","text":""},{"location":"#setting-up-a-development-environment","title":"Setting up a development environment","text":"<p>Before starting your project, your computer needs to be able to interpret all of this code. We have created a separate section which sets out a step-by-step guide to help make this process as straightforward as possible, with information that we have found helpful along the way.</p> <p>Tip</p> <p>If you've never created a Python environment before, or you need a refresher on how we approach this seemingly straightforward task, then this section should be your starting point.</p> <p>Setting up a development environment</p>"},{"location":"#requirements-to-use-the-cookiecutter-template","title":"Requirements to use the cookiecutter template","text":"<ul> <li>Python 3.5+</li> <li>Cookiecutter Python package &gt;= 1.4.0: We recommend that this is installed with Conda's Miniconda Python package management:</li> </ul>"},{"location":"#starting-a-new-project","title":"Starting a new project","text":"<p>Starting a new project is as easy as running this command at the command line. No need to create a directory first, the cookiecutter will do it for you.</p> <pre><code>cookiecutter https://github.com/Prison-Reform-Trust/prt-cookiecutter-data-science\n</code></pre>"},{"location":"#example","title":"Example","text":"<p>Now that you've got your project, you're ready to go! You should do the following:</p> <ul> <li>Check out the directory structure below so you know what's in the project and how to use it.</li> <li>Read the opinions that are baked into the project so you understand best practices and the philosophy behind the project structure.   </li> </ul>"},{"location":"#directory-structure","title":"Directory structure","text":"<pre><code>\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile           &lt;- Makefile with commands like `make create_environment`, \n\u2502                         `make update_environment or `make data`.\n\u251c\u2500\u2500 README.md          &lt;- The top-level README for developers using this project.\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 interim        &lt;- Intermediate data that has been transformed.\n\u2502   \u251c\u2500\u2500 processed      &lt;- The final, canonical data sets for analysis.\n\u2502   \u2514\u2500\u2500 raw            &lt;- The original, immutable data dump.\n\u2502\n\u251c\u2500\u2500 docs               &lt;- A default Sphinx project for adding documentation to this project; \n\u2502                         see sphinx-doc.org for details.\n\u2502\n\u251c\u2500\u2500 notebooks          &lt;- Jupyter notebooks. Naming convention is a number (for ordering),\n\u2502                         the creator's initials, and a short `-` delimited description, e.g.\n\u2502                         `1.0-jqp-initial-data-exploration`.\n\u2502\n\u251c\u2500\u2500 references         &lt;- Data dictionaries, manuals, and all other explanatory materials.\n\u2502\n\u251c\u2500\u2500 reports            &lt;- Generated analysis as HTML, PDF, LaTeX, etc.\n\u2502   \u2514\u2500\u2500 figures        &lt;- Generated graphics and figures to be used in reporting\n\u2502\n\u251c\u2500\u2500 requirements.txt   &lt;- The requirements file for reproducing the analysis environment, e.g.\n\u2502                         generated with `conda list --export &gt; requirements.txt`\n\u2502\n\u251c\u2500\u2500 setup.py           &lt;- makes project pip installable (pip install -e .) so src can be imported\n\u251c\u2500\u2500 src                &lt;- Source code for use in this project.\n\u2502   \u251c\u2500\u2500 __init__.py    &lt;- Makes src a Python module\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 analysis       &lt;- Scripts to process raw data for analysis\n\u2502   \u2502   \u2514\u2500\u2500 process_data.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 data           &lt;- Scripts to download or generate data\n\u2502   \u2502   \u2514\u2500\u2500 make_dataset.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 visualization  &lt;- Scripts to create exploratory and results oriented visualizations\n\u2502       \u2514\u2500\u2500 visualize.py\n\u2502\n\u2514\u2500\u2500 tox.ini            &lt;- tox file with settings for running tox; see tox.readthedocs.io\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>This project is currently in development and borrows heavily on the excellent work of DrivenData and their cookiecutter-data-science template (CCDS). Which we have relied on for a considerable period. The development of this template is also indebted to EasyData and their excellent cookiecutter template which inspired the development of the PRT project, and provided a deeper understanding of how to create reproducible environments; datasets and workflows for data analysis.</p> <p>As such, this project is primarily focused on adapting the project to suit our own organisational needs and isn't actively seeking contributions from outside of the organisation.</p> <p>If you would like to find out more about the DrivenData cookiecutter-data-science template and how to contribute to their project then see their docs for guidelines.</p>"},{"location":"#links-to-related-projects-and-references","title":"Links to related projects and references","text":"<p>Here are some projects and blog posts that have provided a huge amount of information and guidance to inform this cookiecutter template and which you may find useful.</p> <ul> <li>Cookiecutter Data Science (CCDS) - The original template on which this is based.</li> <li>EasyData - A revised implementation of the CCDS template and which triggered the development of this project.</li> <li>Coding for Economists - An excellent blog by Arthur Turrell covering a wide range of topics, from getting your development environment started to suggested workflows; data transformation; designing reproducible analysis and much, much more.</li> <li>Government Analysis Function guidance aka The Duck Book - Another great guidance doc to draw from, which govers guiding principles; modular coding; documentation; version control and loads more.</li> </ul> <p>Finally, a huge thanks to the Cookiecutter project (github), which is helping us all spend less time thinking about and writing boilerplate and more time getting things done.</p>"},{"location":"opinions/","title":"Opinions","text":"<p>The default project structure reflects certain opinions about how to do collaborative data science work. These opinions grew out of our own experiences with what works and what doesn't. Some of these opinions are about workflows, and others are about tools that can make the process easier. These opinions are discussed below.</p>"},{"location":"opinions/#data-analysis-is-a-directed-acyclic-graph","title":"Data analysis is a directed acyclic graph","text":"<p>Don't ever edit your raw data, especially not manually, and especially not in Excel.</p> <p>The most important features of a quality data analysis are correctness and reproducibility\u2014anyone should be able to re-run your analysis using only your code and raw data and produce the same final products. The best way to ensure correctness is to test your analysis code. The best way to ensure reproducibility is to treat your data analysis pipeline as a directed acyclic graph (DAG). This means each step of your analysis is a node in a directed graph with no loops. You can run through the graph forwards to recreate any analysis output, or you can trace backwards from an output to examine the combination of code and data that created it.</p>"},{"location":"opinions/#raw-data-is-immutable","title":"Raw data is immutable","text":"<p>That proper data analysis is a DAG means that raw data must be treated as immutable\u2014it's okay to read and copy raw data to manipulate it into new outputs, but never okay to change it in place. This informs the design of the default <code>data/</code> directory subfolders in which data originates from <code>raw/</code>, intermediate analytical outputs get serialized or cached in <code>interim/</code>, and final products end up in <code>processed/</code> (the number or names of these folders is less important than flow of data between them). </p> <p>Some dos and don'ts that follow from treating data analysis as a DAG:</p> <ul> <li>\u2705 Do write code that moves the raw data through a pipeline to your final analysis.</li> <li>\u2705 Do serialize or cache the intermediate outputs of long-running steps.</li> <li> <p>\u2705 Do make it possible (and ideally, documented and automated) for anyone to reproduce your final data products with only the code in <code>{{ cookiecutter.module_name }}</code> and the data in <code>data/raw/</code>.</p> </li> <li> <p>\u26d4 Don't ever edit your raw data, especially not manually, and especially not in Excel. This includes changing file formats or fixing errors that might break a tool that's trying to read your data file.</p> </li> <li>\u26d4 Don't overwrite your raw data with a newly processed or cleaned version.</li> <li>\u26d4 Don't save multiple versions of the raw data. </li> </ul>"},{"location":"opinions/#data-should-mostly-not-be-kept-in-source-control","title":"Data should (mostly) not be kept in source control","text":"<p>Another consequence of treating data as immutable is that data doesn't need source control in the same way that code does. Therefore, by default, the <code>data/</code> folder is included in the <code>.gitignore</code> file. If you have a small amount of data that rarely changes, you may want to include the data in the repository. GitHub currently warns you if files are over 50MB and rejects any files over 100MB. </p> <p>If you have larger amounts of data, consider storing and syncing with a cloud service like Amazon S3, Azure Blob Storage, or Google Cloud Storage.</p>"},{"location":"opinions/#notebooks-are-for-exploration-and-communication-source-files-are-for-repetition","title":"Notebooks are for exploration and communication, source files are for repetition","text":"<p>Source code is superior for replicability because it is more portable, can be tested more easily, and is easier to code review.</p> <p>Jupyter Notebook and other literate programming tools are very effective for exploratory data analysis because they enable rapid iteration and visualization of results. However, these tools can be less effective for reproducing an analysis. Source code is superior for replicability because it is more portable, can be tested more easily, and is easier to code review. </p> <p>When we use notebooks in our work, we often subdivide the <code>notebooks/</code> folder to keep things organized and legible. For example, <code>notebooks/exploratory/</code> contains initial explorations, whereas <code>notebooks/reports/</code> is more polished work that can be exported as html to the <code>reports/</code> directory. We also recommend that you follow a naming convention that shows the owner and the order the analysis was done in. We use the format <code>&lt;step&gt;-&lt;ghuser&gt;-&lt;description&gt;.ipynb</code> (e.g., <code>0.3-bull-visualize-distributions.ipynb</code>). Since notebooks are challenging objects for source control (e.g., diffs of the <code>json</code> are often not human-readable and merging is near impossible), we recommended not collaborating directly with others on Jupyter notebooks. We also recommend using a tool like <code>nbautoexport</code> to make reviewing changes to notebooks easier. </p>"},{"location":"opinions/#refactor-the-good-parts-into-source-code","title":"Refactor the good parts into source code","text":"<p>Don't write code to do the same task in multiple notebooks. If it's a data preprocessing task, put it in the pipeline at <code>{{ cookiecutter.module_name }}/data/make_dataset.py</code> and load data from <code>data/interim/</code>. If it's useful utility code, refactor it to <code>{{ cookiecutter.module_name }}</code>. Classic signs that you are ready to move from a notebook to source code include duplicating old notebooks to start new ones, copy/pasting functions between notebooks, and creating object-oriented classes within notebooks.</p> <p>We make it easy to refactor notebook code because this template makes your project a Python package by default and installs it locally in the requirements file of your chosen environment manager. This enables you to import your project's source code and use it in notebooks with a cell like the following:</p> <pre><code># OPTIONAL: Load the \"autoreload\" extension so that code can change\n%load_ext autoreload\n\n# OPTIONAL: always reload modules so that as you change code in {{ cookiecutter.module_name }}, it gets loaded\n%autoreload 2\n\nfrom {{ cookiecutter.module_name }}.data import make_dataset\n</code></pre>"},{"location":"opinions/#build-from-the-environment-up","title":"Build from the environment up","text":"<p>The first step in reproducing an analysis is always replicating the computational environment it was run in. You need the same tools, the same libraries, and the same versions to make everything play nicely together.</p> <p>Doing so in Python requires choosing and configuring an environment management tool. The ecosystem for this tooling has evolved a lot in recent years. </p> <p>For data science work, we prefer to use the conda package manager because it also manages non-Python packages, including system library dependencies that you often run into in data science. Our recommended way to install conda is with Miniconda as it is light on resources without all of the bloat of the full-fat Anaconda. Ted Petrou's aka DunderData's helpful blog on this was instructive. </p>"},{"location":"opinions/#keep-secrets-and-configuration-out-of-version-control","title":"Keep secrets and configuration out of version control","text":"<p>You really don't want to leak your Plotly secret key or Microsoft 365 username and password on Github\u2014see the Twelve Factor App principles on this point. Here's one way to do this:</p>"},{"location":"opinions/#store-your-secrets-and-config-variables-in-a-special-file","title":"Store your secrets and config variables in a special file","text":"<p>Create a <code>.env</code> file in the project root folder. Thanks to the <code>.gitignore</code>, this file should never get committed into the version control repository. Here's an example:</p> <pre><code># example .env file\nDATABASE_URL=postgres://username:password@localhost:5432/dbname\nAWS_ACCESS_KEY=myaccesskey\nAWS_SECRET_ACCESS_KEY=mysecretkey\nOTHER_VARIABLE=something\n</code></pre>"},{"location":"opinions/#use-a-package-to-load-these-variables-automatically","title":"Use a package to load these variables automatically.","text":"<p>If you look at the stub script in <code>{{ cookiecutter.module_name }}/data/make_dataset.py</code>, it uses a package called python-dotenv to load up all the entries in this file as environment variables so they are accessible with <code>os.environ.get</code>. Here's an example snippet adapted from the <code>python-dotenv</code> documentation:</p> <pre><code># {{ cookiecutter.module_name }}/data/dotenv_example.py\nimport os\nfrom dotenv import load_dotenv, find_dotenv\n\n# find .env automagically by walking up directories until it's found\ndotenv_path = find_dotenv()\n\n# load up the entries as environment variables\nload_dotenv(dotenv_path)\n\ndatabase_url = os.environ.get(\"DATABASE_URL\")\nother_variable = os.environ.get(\"OTHER_VARIABLE\")\n</code></pre>"},{"location":"opinions/#encourage-adaptation-from-a-consistent-default","title":"Encourage adaptation from a consistent default","text":"<p>To keep this structure broadly applicable for many different kinds of projects, we think the best approach is to be liberal in changing the folders around for your project, but be conservative in modifying the default cookiecutter structure for all projects.</p>"},{"location":"opinions/#examples-of-template-adaptation-and-evolution","title":"Examples of template adaptation and evolution","text":"<p>A project's organizational needs may differ from the start and can change over time. Here are some examples of directions to go in evolving your project structure. </p>"},{"location":"opinions/#example-1-simplifying","title":"Example 1: Simplifying","text":"<p>Some projects don't require multiple sub-directories to organize their module code. When a few python files can effectively accomplish all that is required, flattening folders into files can make things easier to track and maintain. You can see an example of this in our cyfi package. If it's in the template but you don't need it, delete it!</p>"},{"location":"opinions/#example-2-expanding","title":"Example 2: Expanding","text":"<p>By contrast, we've added more folders to organize module code on more complex projects. A good example of this is our zamba package for which we've introduced new folders to handle task-specific portions of the codebase.</p>"},{"location":"opinions/#example-3-re-organizing","title":"Example 3: Re-organizing","text":"<p>On long-running projects, the <code>notebooks</code> folder can get congested. One adaptation we've employed is to add a top-level <code>research/</code> folder (and a corresponding <code>data/research</code> data folder) that contains sub-folders for individual experiments. These sub-folders can contain their own notebooks, code, and even their own Makefiles that inherit from the parent project <code>Makefile</code>.</p>"},{"location":"setting-up-a-development-environment/","title":"Setting up a development environment","text":"<p>This tutorial outlines how you can get up and running managing your Python development environments.</p>"},{"location":"setting-up-a-development-environment/#installation","title":"Installation","text":"<p>We recommend that that you install and configure the following to get started:</p> <ul> <li>Miniconda</li> <li>Git/GitHub</li> <li>Visual Studio Code</li> </ul> <p>Once you have set up your development environment you can move on to Starting a new project.</p>"},{"location":"setting-up-a-development-environment/#miniconda","title":"Miniconda","text":"<p>This is a setup step that you only need to do once per machine. After this is done, you shouldn't need to do this step again. Occasionally it may be necessary to update your requirements.</p>"},{"location":"setting-up-a-development-environment/#installing-miniconda","title":"Installing Miniconda","text":"<ol> <li>Install Miniconda: if you don't already have Miniconda, you'll need to install it following the instructions for your platform (MacOS/Windows/Linux)</li> <li>Open a terminal window</li> <li> <p>To verify that conda is installed, in your terminal window, run: <pre><code>conda --version\n</code></pre>     Conda responds with the version number that you have installed, such as <code>conda 24.1.2</code>.</p> <p>If you get an error message, check the Anaconda website for advice on suggestions on how to resolve.</p> </li> </ol>"},{"location":"setting-up-a-development-environment/#configuring-miniconda","title":"Configuring Miniconda","text":"<p>Set your channel priority to strict by entering in the terminal: <pre><code>conda config --set channel_priority strict\n</code></pre> Then prevent the base Conda environment from automatically activating when you open a terminal window: <pre><code>conda config --set auto_activate_base false\n</code></pre></p>"},{"location":"setting-up-a-development-environment/#creating-a-base-prtdata-environment","title":"Creating a base prtdata environment","text":"<p>Note</p> <p>This section is intended to provide a simple step-by-step guide to creating your development environment. If you want to find out more about managing Conda environments, visit the Conda docs.</p> <p>Create your base prtdata environment with the following requirements: <pre><code>conda create -n prtdata python=3 cookiecutter make\n</code></pre> You've now created a new conda environment named <code>prtdata</code> that you'll use to create all PRT cookiecutter projects. Once this environment has been created, you won't need to create it again.</p> <p>You can now activate your new environment using the following command: <pre><code>conda activate prtdata\n</code></pre> By default, the active environment---the one you are currently using---is shown in parentheses ( ) or brackets [ ] at the beginning of your command prompt: <pre><code>(prtdata) $\n</code></pre> To deactivate your environment enter: <pre><code>conda deactivate prtdata\n</code></pre></p>"},{"location":"setting-up-a-development-environment/#git-and-github","title":"Git and GitHub","text":"<p>Building projects is one of the core parts of being a developer. And Git and GitHub are essential tools you'll use when building projects with others.</p> <p>Having a way to document or track your code helps to make your work organised, and lets you keep track of the changes you've made at different stages. This is what Git lets you do.</p> <p>But what if you're working with other developers or if you work in different locations with different computers, such as at work or at home? This is where GitHub comes in.</p> <p>GitHub provides you with a place to host your code. This makes controlling each version of your project easier and faster, and enables developers to collaborate.</p> <p>When you're using GitHub, you're working with Git beneath the hood.</p>"},{"location":"setting-up-a-development-environment/#installing-git","title":"Installing Git","text":"<p>Git generally comes pre-installed on MacOS and Linux-based systems, but you can always check if you have Git installed in your machine by typing <code>git version</code> in your terminal.</p> <p>Similar to when you checked your Conda installation above, you should receive a response, such as <code>git version 2.28.0</code>. If you don't have Git installed in your computer, you won't get a version.</p> <ol> <li>Install Git: if you don't already have Git, you'll need to install it following the instructions for your platform (MacOS/Windows/Linux)</li> <li>Open terminal and type <code>git version</code> to verify that Git was successfully installed.</li> </ol>"},{"location":"setting-up-a-development-environment/#configuring-git","title":"Configuring Git","text":"<ol> <li>Set your Git username <pre><code>git config --global user.name \"Mona Lisa\"\n</code></pre></li> <li>Set your Git email address <pre><code>git config --global user.email \"YOUR_EMAIL\"\n</code></pre></li> <li>Confirm that you have set these correctly in Git: <pre><code>git config --global user.name\n&gt; Mona Lisa\ngit config --global user.email\n&gt; YOUR_EMAIL\n</code></pre></li> </ol>"},{"location":"setting-up-a-development-environment/#github","title":"GitHub","text":"<ol> <li>Create a GitHub account and follow the prompts.</li> <li>Verify your email address. You should receive an email to the address you signed up with at GitHub.</li> <li>Check your GitHub account is working by logging on and taking a look around.</li> </ol> <p>Tip</p> <p>Git and GitHub can seem a bit intimidating at first, but remember it's just a way of tracking changes and providing effective version control. Check out [XXXXXX] for a separate guide on our preferred GitHub workflow, as well as the extensive docs from both Git and GitHub.</p>"},{"location":"setting-up-a-development-environment/#visual-studio-code","title":"Visual Studio Code","text":"<p>Visual Studio Code aka VS Code is a free and open source integrated development environment (IDE) from Microsoft that is available on all major operating systems.</p> <p>Just like Python itself, VS Code can be extended with packages, and it is those packages, called extensions in this case, that make it so useful. As well as Python, VS Code supports a ton of other languages.</p> <p>There are other IDEs available, but VS Code brings a lot of features together in one app, including Jupyter Notebooks, Git version control, code linting, debugging, and much more. Use it, then try Spyder, you'll thank us later.</p>"},{"location":"setting-up-a-development-environment/#installing-vs-code","title":"Installing VS Code","text":"<p>No need to dwell on this one. Go to the VS Code website and follow the instructions.</p>"},{"location":"setting-up-a-development-environment/#extensions-in-vs-code","title":"Extensions in VS Code","text":"<p>Once you have VS Code installed and opened, navigate to the \u2018extensions\u2019 tab on the left hand side vertical bar of icons (it\u2019s the one that looks like 4 boxes). You\u2019ll need to install the Python extension, which you can search for by using the text box within VS Code\u2019s extensions panel.</p> <p>There are some other extensions it\u2019s useful to have and install (if they aren\u2019t already):</p> Extension Function Jupyter Adds Jupyter notebook support Pylance Adds Python IntelliSense type checking indent-rainbow Makes code indentation easier to read GitHub Repositories Remotely browse and edit any GitHub repository Path Intellisense Adds filename autocompletion Black Formatter Adds formatting support for Python files using the  Black  formatter. Rainbow CSV Rainbow CSV - Highlights columns in comma (.csv) files and more YAML Adds comprehensive YAML Language support (useful for environment.yml files) vscode-icons Adds icons to VS code file explorer, making it easier to find the file you want Shebang Snippets Insert \"shebang\" lines automatically"},{"location":"setting-up-a-development-environment/#hello-world","title":"Hello world!","text":"<p>Whilst a full tutorial on how to use VS Code is outside of the scope of this guide, VS Code's Getting Started tutorial is a good place to learn more.</p> <p>Warning</p> <p>As you have already installed Miniconda, you can ignore the first steps on installing a Python interpreter. There's also no need to create a virtual environment, as you've already created your <code>prtdata</code> conda environment. Open the Command Palette (\u21e7\u2318P), start typing the Python: Select Interpreter command and select the <code>(prtdata)</code> environment from the list.</p>"},{"location":"setting-up-a-development-environment/#youre-ready","title":"You're ready","text":"<p>If you've made it this far then congratulations! You are now ready to start your journey as a developer and move on to the next stage \u2014 Starting a new project</p>"},{"location":"starting-a-new-project/","title":"Starting a new project","text":"<p>So you've already set up your prtdata development environment and now you're ready to get coding!</p>"},{"location":"starting-a-new-project/#create-your-projectrepo","title":"Create your project/repo","text":"<p>The best time to use the prt cookiecutter template is right at the start, when you first create your project/repo. We will assume that you are starting your project from scratch.</p> <p>Warning</p> <p>We recommend using prtdata to create every project you work with so that there is at least a 1:1 ratio between your conda environments and your projects. There are many issues with having more than one repo using the same conda environment, so whatever you do please don't use a monolithic environment to rule them all. For more on this idea see Tip #2 of this talk on building reproducible workflows. </p>"},{"location":"starting-a-new-project/#create-a-project-using-a-prt-cookiecutter-template","title":"Create a project using a PRT cookiecutter template","text":"<ol> <li>Open a terminal window</li> <li>Activate the <code>prtdata</code> environment you created earlier: <code>conda activate prtdata</code></li> <li>Navigate to the location that you'd like your project to located (without creating the project directory, that happens automagically in the next step). For example, if you want my project to be located in <code>/home/&lt;my-repo-name&gt;</code> navigate to <code>/home</code> in this step.</li> <li>Create your project. Run <code>cookiecutter https://github.com/Prison-Reform-Trust/prt-cookiecutter-data-science</code> and fill in the prompts. Note that the repo name that you enter will be the name of the directory that your project lives in.</li> </ol> <p>We've now created a project filled with the PRT cookiecutter template files in <code>&lt;my-repo-name&gt;</code>. </p>"},{"location":"starting-a-new-project/#initialize-the-project-as-a-github-repo","title":"Initialize the project as a GitHub repo","text":"<p>As we have previously explained we'd like to use git to keep track of changes that are made to our project. Now is the best time to initialize the git repo and create the GitHub connection.</p> <ol> <li>Load VS Code and open your project folder</li> <li>Click the 'Source Control' tab () on the left hand side vertical bar of icons (it looks like three circle joined with lines)</li> <li>You should see two options - to Initialize Repository or Publish to GitHub.</li> <li>Initialize Repository - this will create the necessary Git repository metadata files and show your workspace files as untracked changes ready to be staged. </li> <li>Publish to GitHub - this will directly publish your workspace folder to a GitHub repository, allowing you to choose between private and public repositories. You will probably be prompted to authenticate your GitHub account at this stage.</li> </ol> <p>Tip</p> <p>This is just a starting point for using Git and GitHub, there is lots more to explore. We strongly recommend visiting the GitHub and Visual Studio Code getting started tutorials to get the most out of this powerful integration.</p>"},{"location":"starting-a-new-project/#building-your-project-environment","title":"Building your project environment","text":"<p>As we have already set out above, there is a very strong case against having a single monolithic development environment from which you build all of your projects. Whilst no package manager (read Conda) is perfect, there are features that you can make use of to try and avoid what's known as dependency hell.</p>"},{"location":"starting-a-new-project/#environmentyml","title":"environment.yml","text":"<p>One such way that Conda attempts to mitigate against the dreaded: <pre><code>&gt; Solving environment: failed \n</code></pre> is through the use of <code>environment.yml</code> files. These are a simple text file which contains a list of the names of the packages that you want to include in your development environment (dependencies), and may also include the chosen name of that environment and the channels to use to download the dependencies/packages. </p> <p>We're not going to get into much more detail on <code>environment.yml</code> files, but there's a short primer in Conda's docs if you want to find out more..</p>"},{"location":"starting-a-new-project/#how-to-build-your-environment","title":"How to build your environment","text":"<p>Warning</p> <p>If you have not already completed the preceding steps then return to Create your project/repo and follow the steps back to here.</p> <p>So you've now created your PRT cookiecutter project and initialised it as a GitHub repo. The next step is to build your development environment. </p> <ol> <li>Open a terminal window in VS Code (Terminal &gt; New Terminal)</li> <li>Check that the terminal prompt is showing your project's directory  <pre><code>&lt;my-repo-name&gt; $ \n</code></pre></li> <li>Activate <code>prtdata</code> <pre><code>conda activate prtdata\n(prtdata)&lt;my-repo-name&gt; $ \n</code></pre></li> <li>Build your project environment by entering the following command: <pre><code>conda env create --prefix ./envs -f environment.yml\n</code></pre></li> </ol>"},{"location":"starting-a-new-project/#todo-6-add-default-environmentyml-to-cookiecutter-repo","title":"TODO #6 Add default environment.yml to Cookiecutter repo","text":"<p>This command reads the <code>environment.yml</code> file within your project directory and builds a new development environment called <code>./envs</code> within that directory. As Conda sets out in their docs:</p> <p>Specifying a location for an environment</p> <p>Specifying a path to a subdirectory of your project directory when creating an environment has the following benefits:</p> <ul> <li>It makes it easy to tell if your project uses an isolated environment by including the environment as a subdirectory.</li> <li>It makes your project more self-contained as everything, including the required software, is contained in a single project directory.</li> </ul> <p>Once all of the packages have been downloaded and your environment has been created it's time to activate your environment. </p> <p><pre><code>conda activate ./envs\n</code></pre> By doing this you avoid the cognitive load of having to think of a new name for your development environment every time you start a new project. As long as you navigate to your project folder and activate <code>./envs</code> you know you'll be working in that project's dedicated environment.</p> <p>One small downside is that the full filepath to the environment might take up a lot of visible space in your terminal prompt. For example:</p> <p><pre><code>(/Users/USER_NAME/research/data-science/PROJECT_NAME/envs) $\n</code></pre> Whilst the vast majority of your development will take place using VS Code, if this is bothering you, then run the following command and a shortened version will be set:</p> <p><pre><code>`conda config --set env_prompt '({name})'`\n(envs) $\n</code></pre> Let the coding begin!</p>"},{"location":"starting-a-new-project/#exploring-the-default-environment","title":"Exploring the default environment","text":"<p>The prt template project <code>&lt;my-repo-name&gt;</code> is populated with recommended default settings. Defaults that we've developed over time that work as a nice base for most folks who are creating a data science related project. It's a great place to start, and until you're familiar with how and why these defaults work in most cases, we don't recommend messing with them.</p>"},{"location":"starting-a-new-project/#using-your-conda-environment-in-a-jupyter-notebook","title":"Using your conda environment in a Jupyter notebook","text":"<ol> <li>In VS Code press Cmd+Shift+P</li> <li>Type 'Jupyter' in the menu and select 'Create: New Jupyter notebook'</li> <li>Save the new notebook as <code>prtdata-test.ipynb</code> in the 'notebooks' folder of your project</li> <li>Select the <code>envs</code> environment from within the notebook: Click Select Kernel &gt; Python Environments &gt; envs (Python 3.XX) <code>&lt;my-repo-name&gt;</code><code>/envs/bin/python</code>. </li> <li>Check that it's finding the right environment by entering  <code>conda list</code> in a cell and running the cell. The output should include this at the top: <pre><code># packages in environment at &lt;path-to-environment&gt;: \n\n...\n</code></pre>     where the <code>&lt;path-to-environment&gt;</code> ends in <code>&lt;my-repo-name&gt;/envs</code>. </li> </ol>"},{"location":"starting-a-new-project/#exploring-what-makes-up-your-environment","title":"Exploring what makes up your environment","text":"<p>To see what's in your Conda environment, open up the <code>environment.yml</code> file. In it is a list of the packages that are installed in your project's virtual Conda environment.</p> <p>You might notice that the output that was returned when you ran <code>conda list</code> in your Jupyter Notebook was quite a bit longer than the number of packages listed in the <code>environment.yml</code>. That's because there were other dependencies that had to be installed to make the packages in <code>environment.yml</code> run.</p>"},{"location":"starting-a-new-project/#todo-1-include-a-reference-to-lockfiles-here-once-implemented","title":"TODO #1 include a reference to lockfiles here once implemented","text":"<p>We like to think of these two files are representing two different perspectives:</p> <ul> <li><code>environment.yml</code>: The packages that you want. This file is manually maintained.</li> <li><code>environment.&lt;your-architecture&gt;.lock.yml</code>: The packages that you need (to run what you want). This file is autogenerated and updated.</li> </ul>"},{"location":"starting-a-new-project/#default-paths","title":"Default paths","text":"<p>Hardcoded paths such as <code>my_username/folder2/project_abc/downloads/rawdata</code> are a notorious source of reproducibility issues. They require all users to have exactly the same filepath structure. But what if they have a different username, or they have stored their repository in a different folder to yours?</p> <p>One way to avoid these path-related issues is to use a separate mechanism for handling paths. Rather than hardcoding <code>my_username/folder2/project_abc/downloads/rawdata</code> you instead have a local configuration file that points to the location of the directories in your project through code. This means that you can import your local configuration file into your script and call <code>paths['raw_data_path']</code> instead, without worrying that the full filepath is exactly the same as the original author's.</p>"},{"location":"updating-your-environment/","title":"Updating your development environment","text":"<p>We've already created a development environment and now you realise that there's a package that you need or forgot to install\u2014let's customize it.</p>"},{"location":"updating-your-environment/#always-use-the-environmentyml","title":"Always use the <code>environment.yml</code>!","text":"<p>Warning</p> <p>When adding packages to your python environment, do not use <code>pip install</code> or <code>conda install</code> directly. Always edit <code>environment.yml</code> and run <code>conda env update</code> instead. This will go a long way to avoiding the dreaded dependency hell.</p> <p>Your <code>environment.yml</code> file will look something like this: <pre><code>channels:\n  - defaults\n  - conda-forge\ndependencies:\n  - notebook\n  - cookiecutter\n  - black\n  - jupyterlab\n  - lxml\n  - matplotlib\n  - odfpy\n  - poppler\n  - python-dotenv\n  - seaborn\n  - plotly\n  - nbqa\n  - pandas\n  - python-kaleido\n  - chart-studio\n  - ipywidgets\n  - plotly::plotly-geo\n...\n</code></pre> To add any package available from conda, add it to the end of the list and save.</p> <p>Once you've made your edits, run <code>conda env update</code> in the terminal and voila, you're updated.</p>"},{"location":"updating-your-environment/#checking-your-updates-back-into-the-project-repo","title":"Checking your updates back into the project repo","text":"<p>To share your updated environment, check in your <code>environment.yml</code> file so others can use it. We'll follow the EasyData git workflow</p> <ol> <li>Checkout a new branch: <code>git checkout -b update-environment</code></li> <li>Stage the changes: <code>git add -p environment.yml</code> and include the desired changes, discarding the rest</li> <li>Commit the changes: `git commit -m \"update the environment file\"</li> <li>Push to your origin: <code>git push origin update-environment</code></li> <li>Create a pull request (PR) to the main branch</li> <li>Merge this PR: If you look at the PR, the only changes should be to the <code>environment.yml</code> file. If all looks well, merge this PR.</li> <li>Incorporate changes back into your local setup:  <pre><code>git checkout main\ngit fetch origin\ngit merge origin/main\n</code></pre> The local <code>main</code> branch will now have the updated <code>environment.yml</code>. While it seems a bit roundabout here, we recommend always using GitHub PRs to keep track of changes. This allows for the adoption of a multi-user workflow using an <code>upstream</code> seamlessly. </li> </ol>"}]}